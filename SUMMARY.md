# BoatFinder - Complete System Summary

## âœ… What's Built

A fully automated Facebook Marketplace boat scraper that:
1. Searches for boats in Israel every hour
2. Fetches descriptions for new listings only
3. Uses LLM to analyze if boats match your criteria
4. Stores everything in PostgreSQL
5. Logs notifications for boats in your price range
6. Ready to deploy to Vercel

## ğŸ¯ Your Search Criteria (LLM Evaluated)

The system looks for boats matching:
- **Power Category**: ×¢×•×¦××” × (otzma alef - power A)
- **Length**: Up to 7 meters
- **Engine Power**: Up to 150 HP
- **Parking**: Must mention parking (×—× ×™×”/××§×•× ×¢×’×™× ×”/××¨×™× ×”)
- **Ideal Price**: Around â‚ª60,000

**The LLM extracts these specs from Hebrew descriptions and rates each boat 0-10 based on match quality.**

## ğŸ“Š Data Collected

Each listing includes:

### From Facebook Marketplace:
- ID, Title, Price, Original Price
- Location (city, state)
- URL, Delivery types
- Sold/Pending status
- Category ID, Subtitle
- **Description** (fetched separately)

### From LLM Analysis:
- **hasParking** (boolean) - Does it mention parking?
- **llm_rating** (0-10) - How well it matches your criteria
- **llm_reason** (string) - Detailed explanation with extracted specs

## ğŸ”„ Hourly Workflow

```
Every hour:
â”œâ”€ Search 1: "×¡×™×¨×”" in Tel Aviv (250km)
â”œâ”€ Search 2: "×¡×™×¨×ª ×“×™×™×’" in Tel Aviv (250km)
â”‚
For each result:
â”œâ”€ Check if listing ID exists in DB
â”‚  â”œâ”€ If EXISTS â†’ Skip
â”‚  â””â”€ If NEW â†’
â”‚     â”œâ”€ Fetch description (15 second API call)
â”‚     â”œâ”€ Run LLM analysis (extract specs, rate 0-10)
â”‚     â”œâ”€ Save to PostgreSQL
â”‚     â””â”€ If price â‚ª10k-â‚ª100k â†’
â”‚        â””â”€ Log notification (Discord ready)
```

## ğŸ“ Project Structure

```
BoatFinder/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ cron/
â”‚       â””â”€â”€ search-boats.ts    # Hourly cron endpoint
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ db.ts                  # Postgres connection & queries
â”‚   â”œâ”€â”€ filters.ts             # Price filtering logic
â”‚   â”œâ”€â”€ llm-analysis.ts        # Claude analysis
â”‚   â””â”€â”€ schema.sql             # Database schema
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.ts             # Main search function
â”‚   â”œâ”€â”€ parser.ts              # HTML parsing
â”‚   â”œâ”€â”€ types.ts               # TypeScript interfaces
â”‚   â””â”€â”€ index.ts               # Exports
â”œâ”€â”€ vercel.json                # Cron configuration (hourly)
â”œâ”€â”€ DEPLOYMENT.md              # Full deployment guide
â””â”€â”€ VERCEL_SETUP.md            # Quick setup guide
```

## ğŸ—„ï¸ Database Schema

```sql
CREATE TABLE listings (
  id TEXT PRIMARY KEY,
  title TEXT NOT NULL,
  price TEXT NOT NULL,
  price_numeric INTEGER,
  strikethrough_price TEXT,
  city TEXT NOT NULL,
  state TEXT NOT NULL,
  url TEXT NOT NULL,
  delivery_types TEXT,
  is_sold BOOLEAN,
  is_pending BOOLEAN,
  category_id TEXT,
  subtitle TEXT,
  description TEXT,
  has_parking BOOLEAN,       -- LLM extracted
  llm_rating INTEGER,        -- LLM rating 0-10
  llm_reason TEXT,           -- LLM explanation
  search_query TEXT,
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);
```

## ğŸš€ Deployment Checklist

- [ ] Deploy to Vercel (`vercel`)
- [ ] Create Postgres database in Vercel
- [ ] Set environment variables:
  - `CRAWLBASE_TOKEN` (JavaScript token)
  - `ANTHROPIC_API_KEY`
  - `CRON_SECRET` (random string)
  - `POSTGRES_URL` (auto-set by Vercel)
- [ ] Test cron endpoint manually
- [ ] Wait for hourly cron to run
- [ ] Check logs in Vercel dashboard

## ğŸ’° Cost Estimate

**Per month (~720 searches, ~300 new listings):**
- Vercel: Free
- Vercel Postgres: Free (within limits)
- Crawlbase: ~â‚ª200-400 (depends on usage)
- Anthropic API: ~â‚ª50-100 (Claude Haiku)

**Total: ~â‚ª250-500/month**

## ğŸ”® Future Features (Ready to Implement)

- Discord webhook (code ready, just uncomment)
- Web UI to browse listings
- Price range configuration via env vars
- Manual search API endpoint
- Filtering by LLM rating threshold (only notify 8+/10)

## ğŸ§ª Testing Locally

Test the LLM analysis:
```bash
npx tsx src/test-llm-analysis.ts
```

Test the full scraper:
```bash
npx tsx example.ts
```

## ğŸ“ˆ Monitoring

View cron logs: Vercel Dashboard â†’ Functions â†’ search-boats

Manual trigger:
```bash
curl https://your-app.vercel.app/api/cron/search-boats \
  -H "Authorization: Bearer YOUR_CRON_SECRET"
```
